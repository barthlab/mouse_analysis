# Load and format data from .txt files generated by arduino code, and optionally write to a new file
# Last updated: 2024-05-29
# Author: Rachel Swindell

import re
import os
import sys
import datetime
import pandas as pd
import argparse

# open and format animal data
def time_from_file(filename):
    "Convert filename to timestamp."
    dt = re.findall("\d\d_\d\d_\d\d_?~?T_?~?\d\d_\d\d_\d\d", filename)[0]
    dt = dt.replace("_", "-", 2).replace("~", "-", 2).replace("_T_", " ").replace("-T-", " ").replace("_", ":")
    return datetime.datetime.strptime(dt,'%m-%d-%y %H:%M:%S')

def set_noon(t):
    "Get noon on start of training as timestamp"
    return datetime.datetime(t.year, t.month, t.day, 12, 0, 0)

def load_file(filename):
    '''Loads, formats, and returns lick frequecny data as a data frame.
    
    Loads data from csv format. Extracts the start time from the filename and uses it to format timestamps from milliseconds since start of file to datetimes.
    Converts delays at beginning of trial to timedeltas.

    Parameters:
    filename --- path of file to load (string) '''

    data = pd.read_csv(filename, header=None)

    # rename columns
    mp = {0:"timestamp",1:"poke", 2:"lick", 3:"condition code", 4:"delay"}
    if len(data.columns) == 6:
        mp = {0:"timestamp",1:"poke", 2:"lick", 3:"condition code", 4:"delay", 5:"stimulus"}
    data = data.rename(columns=mp)
    
    # convert time columns to correct type
    dt = time_from_file(filename)
    # convert to int in ms to avoid floating point rounding errors in datetime conversion
    data['timestamp'] = (data['timestamp']*1000).astype(int)
    data["timestamp"] = pd.to_datetime(data["timestamp"], unit="ms", origin=dt)
    data["delay"] = pd.to_timedelta(data["delay"], unit="ms")

    # get noon for timebin alignment
    data['offset'] = set_noon(dt)

    return data

def get_trials(trial):
    '''Generates a pandas Series of x of length len.
    
    Used to label all samples in a trial with the trial number.

    Parameters
    trial --- (value, length)'''
    (x, len) = trial
    # generates a list of x of length len
    return pd.Series([x for i in range(len)], dtype="int")

def enumerate_trials(data, start=0):
    '''Labels each sample in a trial with the trial type and the trial number, numbering trials consecutively from `start`.
    The number of trials, and the number of each trial type, matches v16 of the matlab analysis. Returns the input dataframe with labeled trials.
    '''
    # get first sample in each trial
    # first sample, all samples where current water code is not 7 (timeout) and previous is 7,  last sample
    trial_boundary = pd.concat([data.iloc[[0]], data[(data["condition code"]!=7) & (data["condition code"].shift() == 7)], data.iloc[[data.shape[0]-1]]])
    # indexes of first sample in each trial
    trial_boundary_indicies = pd.Series(trial_boundary.index)

    # get number of samples per trial 
    # (difference in index number between previous sample and current sample)
    num_samples = trial_boundary_indicies.diff().fillna(0).astype('int').tolist()
    # label with number of samples with index
    trial_count = pd.Series(enumerate(num_samples))

    # label all samples in a trial with its trial number
    trial_count = trial_count.map(get_trials).explode()[1:]
    trial_count.index = range(0, trial_count.shape[0])
    trial_count = trial_count + start
    
    # add trial labels to data
    data["trial no"] = trial_count
    # set trial no for last sample
    data.loc[data.shape[0] - 1, "trial no",] = trial_boundary.shape[0] - 1 + start

    return data

def label_trials_stimulus(data):
    '''Label samples as stimulus or blank.'''
    data["stimulus"] = data["stimulus"].replace({0:"blank", 1:"stimulus"})
    return data

def label_trials_water(data):
    '''Return data with all trials labeled as water or no water.
    
    Trial types are decided by the code in the 3rd column (if the trial contains a 3, it is a water trial). 
    Requires trials to already be grouped individually and labeled with trial number.
    '''
    # get all trials labeled with a 3 (water trials)
    go = data.groupby(["trial no"]).filter(lambda x: (x["condition code"]==3).any())
    # label all these trials as water
    go["water"] = ["water" for i in range(go.shape[0])]
    data["water"] = go["water"]
    # label remaning trials as blank
    data["water"] = data["water"].fillna(value="no water")
    return data

def label_trials(data):
    '''Label trials with both stimulus or blank and water or no water distinction.'''
    data = label_trials_water(data)
    if "stimulus" in data.columns:
        data = label_trials_stimulus(data)
    else:
        data["stimulus"] = data["water"]
        data["stimulus"] = data["stimulus"].replace({"water":"stimulus", "no water":"blank"})
    data["type"] = data["water"] + " & " + data["stimulus"]
    return data


def drop_last_sample(data):
    '''Drops last sample of each trial. This has the same timestamp as 
    the second-to-last sample and only indicates the trial is over.'''
    data = data[data.groupby(["animal", "trial no"]).cumcount(ascending=False) > 0]
    return data

def make_animal_df(andir, metadata, animal_name, meta_vals):
    '''Load and format all data files for one animal and return as a data frame.

    Requires:
     --- `andir` only contains data files (may contain directories but not other files)
     --- metadata file is provided
     --- metadata contains animal ID in column named "Animal ID"
     --- metadata contains acclimation time in column named "acc"

    If name of animal in metadata file does not match `animal_name` no data will
    be loaded. Column names in addition to "acc" can be provided to load metadata 
    from those columns in the order they are provided. Prints a warning message
    for all additional columns that were provided but had no data, and returns
    those columns as NaN. For animals that have training split into multiple files,
    the entire filename except for the timestamp _must_ be identical, or they
    will not be loaded in the correct order.

    Parameters:
    andir --- directory containing the data files for that animal (and no other files)
    metadata --- dataframe containing metadata about an animal associated with animal id
                 if animal id column is not named exactly 'Animal ID', will fail
    animal_name --- ID code of animal
    meta_vals --- list of columns to include metadata from. Must include 'acc' for length of acclimation (in days).'''
    
    if metadata.empty:
        print("Must include metadata file including at least acclimation time")
        sys.exit(1)
    if 'acc' not in metadata.columns:
        print("Column indicating acclimation time must be named 'acc'")
        sys.exit(1)
    if 'acc' not in meta_vals:
        print("Must include acc in list of columns to be selected from metadata")
        sys.exit(1)

    fs = os.listdir(andir)
    # ensure files concatenated in time order
    # only works if rest of filename is identical
    fs.sort()
    animal = []        
    # load all files in an animal's folder
    start = 0
    for f in fs:
        f_path = andir + "\\" + f
        if (os.path.isfile(f_path)): 
            try:     
                data = load_file(f_path)
            except UnicodeDecodeError:
                print(f"Check that only behavior files are in {andir}")
                sys.exit(1)
            # labeling trials
            data = enumerate_trials(data, start)
            start = data.tail(1)["trial no"].reset_index(drop=True)[0]
            data = label_trials(data)
            animal.append(data)
    # load metadata
    if animal == []:
        print(f"{animal_name} has no behavior files")
        return pd.DataFrame()
    else:
        animal = pd.concat(animal, ignore_index=True)
        try:
            metadata[metadata["Animal ID"] == animal_name].empty
        except KeyError:
            print("Animal ID column must be named 'Animal ID'")
            sys.exit(1)
        if not (metadata[metadata["Animal ID"] == animal_name].empty):
            an_meta =  metadata[metadata["Animal ID"] == animal_name].reset_index()
            fail_keys = []
            for key in meta_vals:
                try:
                    animal[key] = an_meta.loc[0, key]
                    if an_meta.loc[:,key].isna().any():
                        if key == 'acc':
                            print(f'Acclimation time (required) is missing for {animal_name}. Skipping animal.')
                            return pd.DataFrame()
                        else:
                            fail_keys += [key]
                except KeyError:
                    fail_keys += [key]
                    if key == 'acc':
                        print("Metadata file must include acclimation time in column named 'acc'.")
                        sys.exit(1)
            if len(fail_keys) > 0:
                print("%s %s not in metadata file" % (animal_name, ', '.join(fail_keys)))      
        else:
            print(f"{animal_name}: no metadata - must have at least acclimation time. Skipping animal.")
            return pd.DataFrame()
        
        animal["animal"] = animal_name
        animal['offset'] = animal.loc[0, 'offset']
        animal[fail_keys] = pd.NA
        # one-hot encode lick frequency
        animal.loc[animal["lick"] == 2,"lick"] = 1
        # convert acclimation time to timedelta
        animal.loc[:, "acc"] = pd.to_timedelta(animal["acc"], unit="day")
        animal = drop_last_sample(animal)
        return animal

def make_condition_df(condir, condition, metadata, meta_vals):
    '''Load and format all data files for all animals in a condition, and return as a data frame.
    
    Requires all animals to be included to be in the same directory, which should have no other folders or files in it.
    
    Parameters:
    condir --- path of directory containing animal subdirectories which contain data files
    condition --- name of condition
    metadata --- data frame containing relevant metadata about animals in condition (can contain other information as well)
    meta_vals --- list of colums to include metadata from. Must include 'acc' for length of acclimation (in days).'''
    
    andirs = os.listdir(condir)
    if len(andirs) == 0:
        print('No animals in condition.')
        return pd.DataFrame()
    animals = []
    for animal_name in andirs:
        animal_path = condir + "\\" + animal_name
        animals.append(make_animal_df(animal_path, metadata, animal_name, meta_vals))
    animals = pd.concat(animals, ignore_index=True)
    animals["condition"] = condition
    return animals

def run_formatter(files, as_conditions, output, add, metadata, names, columns):
    '''
    Returns formatted training data as a data frame, and writes that data to the
    output file if provided. Can format one or more condition directiories, or one or more animals.
    If multiple animals are being loaded at the same time and `names` is length 1,
    all animals are given the same condition name. If adding to a file of previously
    formatted data, will take the set of columns in the previously formatted data.

    Parameters:
    files --- list of animal or condition director(ies) to format
    as_conditions --- boolean indicating whether `files` is animal or condition directories
    output --- path to write formatted output to
    add --- boolean indicating whether to overwrite or add to `output` file
    metadata --- path to metadata file
    names --- list of names corresponding to provided files
    columns --- set of column names to load from metadata file'''
    cols = ['acc']
    if columns:
        cols = cols + columns

    # treat provided list of files as condition directories containing animal subdirectories
    if as_conditions:
        if not len(files) == len(metadata):
            print(f'Number of condition directories ({len(files)}) and corresponding metadata files ({len(metadata)}) must match.')
            sys.exit(1)
        if names:
            if not len(files) == len(names):
                print('Number of conditions and provided names must match.')
                sys.exit(1)
        conds = []
        for i in range(len(files)):
            if names:
                name = names[i]
            # if names are not provided: name condition with directory name
            else:
                name = files[i].split('\\')[-1]
            print(f'Formatting {name}')
            meta_df = pd.read_excel(metadata[i])
            animals = make_condition_df(files[i], name, meta_df, cols)
            conds = conds + [animals]
        conds = pd.concat(conds, ignore_index=True)
        out = conds
    # treat provided list of files as animal directories containing training data
    else:
        if not names or len(names) != 1:
            print('Must provide exactly one name to load one condition')
            sys.exit(1)
        if not len(metadata) == 1 or not (len(metadata) == len(files)):
            print('Provide one metadata file for all animals, or number of animals and metadata files must match.')
            sys.exit(1)

        animals = []
        meta_df = pd.read_excel(metadata[0])
        for i in range(len(files)):
            if len(metadata) > 1:
                meta_df = pd.read_excel(metadata[i])
            # name animal with directory name
            an_name = files[i].split('\\')[-1].split('/')[-1]
            animal = make_animal_df(files[i], meta_df, an_name, cols)
            if len(names) != 1:
                animal['condition'] = names[i]
            animals.append(animal)
        animals = pd.concat(animals, ignore_index=True)
        if len(names) == 1:  
            animals['condition'] = names[0]
        out = animals

    # write formatted data to file (.txt only)
    if output:
        if out.empty:
            print('No data to %s.' % ('add' if add else 'write'))
            return out
        
        fmt = output.split('.')[-1]
        if fmt == 'txt':
            print('%s formatted data to %s' % ('Adding' if add else 'Writing', output))
            mode = 'a' if add else 'w'
            for c in columns:
                if c not in out.columns:
                    out[c] = pd.NA
            if add:
                with open(output) as f:
                    columns = f.readline().strip('\n').split(',')
                out = out[columns]
            else:
                out = out[['condition','animal','timestamp','poke','lick','condition code','delay','offset','trial no','water','stimulus','type', 'acc'] + columns]
            out.to_csv(output, index=False, mode=mode, 
                header=(not add))
        else:
            print(f'Output format .{fmt} is not supported.')
    return out


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=
        '''Load and format lick frequency data generated by arduino code. 
        Can load one animal, all animals for a condition, 
        or all animals across multiple conditions.'''
    )
    parser.add_argument('files', nargs='+',help='Input animal or condition directories')
    parser.add_argument('-c', action='store_true', help='If present, treat the provided directories as conditions')
    parser.add_argument('-o', metavar='OUT',help='File to output loaded data')
    parser.add_argument('-a', '--add', action='store_true',help='If provided, append additional animals to output file. Default behavior is to overwrite data in output file.')
    parser.add_argument('-m','--metadata', nargs='+', required=True,
                        help='Set of metadata excel sheets. If multiple directories are provided, order of metadata files must match order directories are provided in.')
    parser.add_argument('-n', '--names', nargs='+',help='Name(s) of conditions to load.')
    parser.add_argument('-k', '--columns', nargs='+', help='Set of columns in metadata file to keep.')
    args = parser.parse_args()

    run_formatter(args.files, args.c, args.o, args.add, args.metadata, args.names, args.columns)

    